diff --git a/hw1/cs285/infrastructure/utils.py b/hw1/cs285/infrastructure/utils.py
index 5a884aa..23c0a51 100644
--- a/hw1/cs285/infrastructure/utils.py
+++ b/hw1/cs285/infrastructure/utils.py
@@ -29,6 +29,8 @@ def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('
         # use the most recent ob to decide what to do
         obs.append(ob)
         ac = policy.get_action(ob) # HINT: query the policy's get_action function
+        print(ac)
+        raise
         ac = ac[0]
         acs.append(ac)
 
diff --git a/hw2/cs285/infrastructure/rl_trainer.py b/hw2/cs285/infrastructure/rl_trainer.py
index 280c17d..db6123d 100644
--- a/hw2/cs285/infrastructure/rl_trainer.py
+++ b/hw2/cs285/infrastructure/rl_trainer.py
@@ -141,7 +141,7 @@ class RL_Trainer(object):
 
     def collect_training_trajectories(self, itr, load_initial_expertdata, collect_policy, batch_size):
         # TODO: GETTHIS from HW1
-        if itr == 0:
+        if itr == 0 and load_initial_expertdata:
             with open(load_initial_expertdata, "rb") as f:
                 loaded_paths = pickle.load(f)
             return loaded_paths, 0, None
