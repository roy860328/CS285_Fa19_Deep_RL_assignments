/home/cilab/anaconda3/envs/cs285/lib/python3.6/site-packages/wandb/__init__.py:1082: ResourceWarning: unclosed file <_io.BufferedWriter name=7>
  _init_headless(run)
/home/cilab/anaconda3/envs/cs285/lib/python3.6/site-packages/wandb/__init__.py:1082: ResourceWarning: unclosed file <_io.BufferedWriter name=9>
  _init_headless(run)
/home/cilab/anaconda3/envs/cs285/lib/python3.6/site-packages/wandb/wandb_config.py:152: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/cilab/Desktop/homework_fall2019/hw1/wandb/run-20200221_072714-q5qijixb/config.yaml' mode='r' encoding='UTF-8'>
  self._load_file(path)
Loading expert policy from... cs285/policies/experts/Ant.pkl
obs (1, 111) (1, 111)
Done restoring expert policy...


********** Iteration 0 ************

Training agent using sampled data from replay buffer...

 loss: 1.1280677318572998

 loss: 1.2767853736877441

 loss: 1.1331703662872314

 loss: 1.1604443788528442

 loss: 1.1680599451065063

 loss: 1.1197190284729004

 loss: 1.0541787147521973

 loss: 1.0822744369506836

 loss: 1.0652375221252441

 loss: 1.1739604473114014

 loss: 0.932535707950592

 loss: 1.1764435768127441

 loss: 0.976151704788208

 loss: 1.0228476524353027

 loss: 0.9749296307563782

 loss: 0.9632665514945984

 loss: 0.9763157367706299

 loss: 0.8985472321510315

 loss: 0.9033102989196777

 loss: 0.903206467628479

 loss: 0.8613049387931824

 loss: 0.8667954802513123

 loss: 0.9617214798927307

 loss: 0.918444812297821

 loss: 0.8791822195053101

 loss: 0.8223896622657776

 loss: 0.8204531669616699

 loss: 0.8141068816184998

 loss: 0.786637544631958

 loss: 0.8436481952667236

 loss: 0.8156358599662781

 loss: 0.8132545948028564

 loss: 0.8047754168510437

 loss: 0.8263766765594482

 loss: 0.7239691019058228

 loss: 0.7101055979728699

 loss: 0.7065473794937134

 loss: 0.7424267530441284

 loss: 0.786777675151825

 loss: 0.7395153641700745

 loss: 0.6980445981025696

 loss: 0.6629020571708679

 loss: 0.7349472641944885

 loss: 0.7358880639076233

 loss: 0.6678739190101624

 loss: 0.6628392934799194

 loss: 0.7337242364883423

 loss: 0.6870811581611633

 loss: 0.7014833092689514

 loss: 0.6454662084579468

 loss: 0.7561370730400085

 loss: 0.6458320021629333

 loss: 0.7169786095619202

 loss: 0.6828227043151855

 loss: 0.6842811703681946

 loss: 0.5894054770469666

 loss: 0.6065223217010498

 loss: 0.6754613518714905

 loss: 0.6696478128433228

 loss: 0.6818932294845581

 loss: 0.6354479193687439

 loss: 0.5613864660263062

 loss: 0.6256363391876221

 loss: 0.5208516120910645

 loss: 0.5870108604431152

 loss: 0.551876962184906

 loss: 0.6049090623855591

 loss: 0.6006307005882263

 loss: 0.6655486822128296

 loss: 0.5797477960586548

 loss: 0.5742858052253723

 loss: 0.5439781546592712

 loss: 0.5860760807991028

 loss: 0.5555674433708191

 loss: 0.5959796905517578

 loss: 0.536597728729248

 loss: 0.5617238879203796

 loss: 0.5017452836036682

 loss: 0.5498638153076172

 loss: 0.5053598284721375

 loss: 0.574718177318573

 loss: 0.5057634711265564

 loss: 0.5526372194290161

 loss: 0.5148419141769409

 loss: 0.4960203170776367

 loss: 0.5146228671073914

 loss: 0.5002686381340027

 loss: 0.5322866439819336

 loss: 0.48309531807899475

 loss: 0.46876534819602966

 loss: 0.48899781703948975

 loss: 0.5238820910453796

 loss: 0.46366965770721436

 loss: 0.4477759301662445

 loss: 0.4812396168708801

 loss: 0.4736795127391815

 loss: 0.4721772074699402

 loss: 0.49353423714637756

 loss: 0.4463929235935211

 loss: 0.47998592257499695

 loss: 0.42925238609313965

 loss: 0.42381545901298523

 loss: 0.46835392713546753

 loss: 0.4679577648639679

 loss: 0.4629513919353485

 loss: 0.4451075494289398

 loss: 0.442146897315979

 loss: 0.4442301094532013

 loss: 0.43469154834747314

 loss: 0.46999451518058777

 loss: 0.432107537984848

 loss: 0.49102485179901123

 loss: 0.4537406861782074

 loss: 0.4433434307575226

 loss: 0.4099629521369934

 loss: 0.4041332006454468

 loss: 0.4107586145401001

 loss: 0.39069458842277527

 loss: 0.3985424041748047

 loss: 0.4677168130874634

 loss: 0.4190688729286194

 loss: 0.416666179895401

 loss: 0.3582663834095001

 loss: 0.3855530619621277

 loss: 0.39250892400741577

 loss: 0.4047488272190094

 loss: 0.37701377272605896

 loss: 0.412068635225296

 loss: 0.40019458532333374

 loss: 0.3938940465450287

 loss: 0.3900706470012665

 loss: 0.36688193678855896

 loss: 0.39019909501075745

 loss: 0.3997424244880676

 loss: 0.3877371847629547

 loss: 0.40043723583221436

 loss: 0.34637174010276794

 loss: 0.38701385259628296

 loss: 0.3694112300872803

 loss: 0.3890598714351654

 loss: 0.3459184169769287

 loss: 0.37442412972450256

 loss: 0.35138702392578125

 loss: 0.3857290744781494

 loss: 0.3913794755935669

 loss: 0.3436656892299652

 loss: 0.33787021040916443

 loss: 0.37613120675086975

 loss: 0.32648757100105286

 loss: 0.3233857750892639

 loss: 0.35549792647361755

 loss: 0.31974735856056213

 loss: 0.3180239498615265

 loss: 0.3270161747932434

 loss: 0.35504016280174255

 loss: 0.31899604201316833

 loss: 0.3169851303100586

 loss: 0.34903404116630554

 loss: 0.3287234902381897

 loss: 0.3462572395801544

 loss: 0.291105180978775

 loss: 0.3204061985015869

 loss: 0.327465295791626

 loss: 0.3249659836292267

 loss: 0.31434500217437744

 loss: 0.28882044553756714

 loss: 0.3342874050140381

 loss: 0.3163587152957916

 loss: 0.2985226511955261

 loss: 0.28797662258148193

 loss: 0.3368602395057678

 loss: 0.32894080877304077

 loss: 0.30806368589401245

 loss: 0.28662383556365967

 loss: 0.2927587628364563

 loss: 0.30101683735847473

 loss: 0.31645500659942627

 loss: 0.2984151542186737

 loss: 0.29002997279167175

 loss: 0.2927548289299011

 loss: 0.2784591019153595

 loss: 0.3004723787307739

 loss: 0.27318036556243896

 loss: 0.29216375946998596

 loss: 0.30640479922294617

 loss: 0.2834298014640808

 loss: 0.2876923680305481

 loss: 0.279060035943985

 loss: 0.2642345130443573

 loss: 0.2796742022037506

 loss: 0.2680921256542206

 loss: 0.29079386591911316

 loss: 0.27959948778152466

 loss: 0.2773726284503937

 loss: 0.27814269065856934

 loss: 0.29496142268180847

 loss: 0.2615549564361572

 loss: 0.2842276990413666

 loss: 0.27965638041496277

 loss: 0.28736937046051025

 loss: 0.26325252652168274

 loss: 0.25272825360298157

 loss: 0.2860598862171173

 loss: 0.24917900562286377

 loss: 0.28719258308410645

 loss: 0.22836889326572418

 loss: 0.28652963042259216

 loss: 0.24914783239364624

 loss: 0.23955905437469482

 loss: 0.24370110034942627

 loss: 0.2398063689470291

 loss: 0.23696422576904297

 loss: 0.26666465401649475

 loss: 0.25796714425086975

 loss: 0.25114670395851135

 loss: 0.27934378385543823

 loss: 0.25092440843582153

 loss: 0.23888152837753296

 loss: 0.2548825442790985

 loss: 0.25673890113830566

 loss: 0.2476571649312973

 loss: 0.2524924576282501

 loss: 0.24746032059192657

 loss: 0.226566880941391

 loss: 0.22296839952468872

 loss: 0.2264019399881363

 loss: 0.23943796753883362

 loss: 0.241714209318161

 loss: 0.23268520832061768

 loss: 0.23130013048648834

 loss: 0.23794181644916534

 loss: 0.22163112461566925

 loss: 0.22488830983638763

 loss: 0.21104192733764648

 loss: 0.23257942497730255

 loss: 0.21514679491519928

 loss: 0.23375028371810913

 loss: 0.21770067512989044

 loss: 0.21626204252243042

 loss: 0.23410724103450775

 loss: 0.21671082079410553

 loss: 0.23662441968917847

 loss: 0.21840178966522217

 loss: 0.23202024400234222

 loss: 0.22385555505752563

 loss: 0.22941820323467255

 loss: 0.21674945950508118

 loss: 0.20203179121017456

 loss: 0.20659267902374268

 loss: 0.22286123037338257

 loss: 0.20530053973197937

 loss: 0.20930320024490356

 loss: 0.21321827173233032

 loss: 0.21967658400535583

 loss: 0.20073401927947998

 loss: 0.1963474303483963

 loss: 0.2120887041091919

 loss: 0.1944122314453125

 loss: 0.18967685103416443

 loss: 0.21360903978347778

 loss: 0.20558887720108032

 loss: 0.18556548655033112

 loss: 0.21131259202957153

 loss: 0.19099567830562592

 loss: 0.18917246162891388

 loss: 0.19552284479141235

 loss: 0.19294188916683197

 loss: 0.19657017290592194

 loss: 0.18716353178024292

 loss: 0.1859578937292099

 loss: 0.19876213371753693

 loss: 0.19563648104667664

 loss: 0.1919933557510376

 loss: 0.19630207121372223

 loss: 0.205458864569664

 loss: 0.19280385971069336

 loss: 0.2038668841123581

 loss: 0.19083766639232635

 loss: 0.18824534118175507

 loss: 0.20688900351524353

 loss: 0.19643281400203705

 loss: 0.19321319460868835

 loss: 0.18308262526988983

 loss: 0.17686668038368225

 loss: 0.20439139008522034

 loss: 0.1807045191526413

 loss: 0.19696976244449615

 loss: 0.18029151856899261

 loss: 0.18485145270824432

 loss: 0.17246250808238983

 loss: 0.17197637259960175

 loss: 0.17137780785560608

 loss: 0.1792990118265152

 loss: 0.16845634579658508

 loss: 0.171452134847641

 loss: 0.17276373505592346

 loss: 0.18492434918880463

 loss: 0.1893850862979889

 loss: 0.19064456224441528

 loss: 0.18789440393447876

 loss: 0.18152770400047302

 loss: 0.1969689130783081

 loss: 0.17053917050361633

 loss: 0.18049071729183197

 loss: 0.18268893659114838

 loss: 0.17561694979667664

 loss: 0.158797487616539

 loss: 0.1662791669368744

 loss: 0.16692371666431427

 loss: 0.16375485062599182

 loss: 0.1653306633234024

 loss: 0.16137748956680298

 loss: 0.1521122306585312

 loss: 0.16824227571487427

 loss: 0.16736485064029694

 loss: 0.14595022797584534

 loss: 0.17182454466819763

 loss: 0.16896085441112518

 loss: 0.16365891695022583

 loss: 0.1658061146736145

 loss: 0.16761566698551178

 loss: 0.16351719200611115

 loss: 0.1578470766544342

 loss: 0.1641959697008133

 loss: 0.15453559160232544

 loss: 0.16722767055034637

 loss: 0.1526634246110916

 loss: 0.1503511667251587

 loss: 0.1490325927734375

 loss: 0.16284817457199097

 loss: 0.15503691136837006

 loss: 0.1575038731098175

 loss: 0.15542063117027283

 loss: 0.14441557228565216

 loss: 0.15120702981948853

 loss: 0.15104541182518005

 loss: 0.15051843225955963

 loss: 0.15429836511611938

 loss: 0.16484203934669495

 loss: 0.14897367358207703

 loss: 0.14036470651626587

 loss: 0.15532897412776947

 loss: 0.1602550894021988

 loss: 0.1493183821439743

 loss: 0.16264794766902924

 loss: 0.1366155594587326

 loss: 0.1550653576850891

 loss: 0.1469971239566803

 loss: 0.15692739188671112

 loss: 0.14471842348575592

 loss: 0.1555125117301941

 loss: 0.15539340674877167

 loss: 0.14353986084461212

 loss: 0.14615802466869354

 loss: 0.15526548027992249

 loss: 0.13564251363277435

 loss: 0.14415046572685242

 loss: 0.1451442390680313

 loss: 0.13707071542739868

 loss: 0.1520988643169403

 loss: 0.13372696936130524

 loss: 0.13980884850025177

 loss: 0.13524553179740906

 loss: 0.13791953027248383

 loss: 0.13374407589435577

 loss: 0.14058499038219452

 loss: 0.13827206194400787

 loss: 0.1415439397096634

 loss: 0.14367684721946716

 loss: 0.13876916468143463

 loss: 0.1459091156721115

 loss: 0.1420859545469284

 loss: 0.12748682498931885

 loss: 0.1355656236410141

 loss: 0.13782894611358643

 loss: 0.13591161370277405

 loss: 0.13457275927066803

 loss: 0.12738674879074097

 loss: 0.1437090039253235

 loss: 0.1421704888343811

 loss: 0.14754018187522888

 loss: 0.1412838250398636

 loss: 0.13399621844291687

 loss: 0.1349215805530548

 loss: 0.1316634714603424

 loss: 0.12165254354476929

 loss: 0.13633722066879272

 loss: 0.14002887904644012

 loss: 0.14365370571613312

 loss: 0.1337539404630661

 loss: 0.1307683140039444

 loss: 0.11939097195863724

 loss: 0.11919782310724258

 loss: 0.13010963797569275

 loss: 0.13180279731750488

 loss: 0.12842199206352234

 loss: 0.11282337456941605

 loss: 0.11877842992544174

 loss: 0.12006760388612747

 loss: 0.12669679522514343

 loss: 0.1198955625295639

 loss: 0.12633176147937775

 loss: 0.1280236840248108

 loss: 0.11937293410301208

 loss: 0.11249218136072159

 loss: 0.12085872888565063

 loss: 0.11461739242076874

 loss: 0.1202186793088913

 loss: 0.12810726463794708

 loss: 0.11938676983118057

 loss: 0.12308789044618607

 loss: 0.1129128634929657

 loss: 0.12461629509925842

 loss: 0.12335880100727081

 loss: 0.11582405865192413

 loss: 0.110003262758255

 loss: 0.12268777936697006

 loss: 0.12066860496997833

 loss: 0.1226181611418724

 loss: 0.11439646035432816

 loss: 0.11422941088676453

 loss: 0.12370657175779343

 loss: 0.11993572115898132

 loss: 0.12602423131465912

 loss: 0.11458323150873184

 loss: 0.11819325387477875

 loss: 0.12611199915409088

 loss: 0.11346811056137085

 loss: 0.10834693163633347

 loss: 0.10502937436103821

 loss: 0.11153867095708847

 loss: 0.10973677784204483

 loss: 0.12514522671699524

 loss: 0.11165183037519455

 loss: 0.11831651628017426

 loss: 0.11864126473665237

 loss: 0.11624849587678909

 loss: 0.11186352372169495

 loss: 0.11972556263208389

 loss: 0.1087905615568161

 loss: 0.10046523809432983

 loss: 0.11163606494665146

 loss: 0.11757303029298782

 loss: 0.11031890660524368

 loss: 0.10471870750188828

 loss: 0.11511731892824173

 loss: 0.12112461030483246

 loss: 0.10983176529407501

 loss: 0.10069246590137482

 loss: 0.10205568373203278

 loss: 0.11645422130823135

 loss: 0.09941177070140839

 loss: 0.10906509310007095

 loss: 0.10824796557426453

 loss: 0.1096879169344902

 loss: 0.10462921112775803

 loss: 0.10490505397319794

 loss: 0.10168363898992538

 loss: 0.10785306990146637

 loss: 0.09957589954137802

 loss: 0.11143402755260468

 loss: 0.11175449192523956

 loss: 0.10751727968454361

 loss: 0.10043293237686157

 loss: 0.09924384206533432

 loss: 0.10517078638076782

 loss: 0.10119041800498962

 loss: 0.0927782952785492

 loss: 0.10083357989788055

 loss: 0.0991746336221695

 loss: 0.10427990555763245

 loss: 0.09430278092622757

 loss: 0.10670235753059387

 loss: 0.10718537122011185

 loss: 0.10331014543771744

 loss: 0.10209972411394119

 loss: 0.10846751928329468

 loss: 0.0976155549287796

 loss: 0.09638184309005737

 loss: 0.10348257422447205

 loss: 0.10715677589178085

 loss: 0.09767603129148483

 loss: 0.10624844580888748

 loss: 0.09103032946586609

 loss: 0.09862266480922699

 loss: 0.09390043467283249

 loss: 0.10160322487354279

 loss: 0.10593258589506149

 loss: 0.09816327691078186

 loss: 0.10441787540912628

 loss: 0.0973627120256424

 loss: 0.09393712878227234

 loss: 0.0966903418302536

 loss: 0.10150179266929626

 loss: 0.09229769557714462

 loss: 0.0924566239118576

 loss: 0.09787721931934357

 loss: 0.0982469767332077

 loss: 0.09289899468421936

 loss: 0.10538175702095032

 loss: 0.0861293226480484

 loss: 0.10906669497489929

 loss: 0.09564986824989319

 loss: 0.09488176554441452

 loss: 0.08767145127058029

 loss: 0.09585213661193848

 loss: 0.10174155980348587

 loss: 0.10064107924699783

 loss: 0.08947119116783142

 loss: 0.08781927078962326

 loss: 0.09193475544452667

 loss: 0.09182407706975937

 loss: 0.0903489962220192

 loss: 0.09830489754676819

 loss: 0.09981441497802734

 loss: 0.09855657815933228

 loss: 0.08687055855989456

 loss: 0.09135805815458298

 loss: 0.09345825016498566

 loss: 0.09043694287538528

 loss: 0.09799034148454666

 loss: 0.08916831761598587

 loss: 0.08365627378225327

 loss: 0.09157965332269669

 loss: 0.08845914900302887

 loss: 0.09067347645759583

 loss: 0.09039276838302612

 loss: 0.08882573992013931

 loss: 0.0848451629281044

 loss: 0.08954236656427383

 loss: 0.09535081684589386

 loss: 0.08575845509767532

 loss: 0.09046804159879684

 loss: 0.09048721194267273

 loss: 0.08953393995761871

 loss: 0.09103847295045853

 loss: 0.08742246776819229

 loss: 0.08734689652919769

 loss: 0.09043756127357483

 loss: 0.08753099292516708

 loss: 0.08657349646091461

 loss: 0.08982556313276291

 loss: 0.09292107820510864

 loss: 0.08888224512338638

 loss: 0.08889079093933105

 loss: 0.08691669255495071

 loss: 0.08071195334196091

 loss: 0.08230973035097122

 loss: 0.08091476559638977

 loss: 0.08991994708776474

 loss: 0.08357885479927063

 loss: 0.07817499339580536

 loss: 0.08191278576850891

 loss: 0.08684933930635452

 loss: 0.09090499579906464

 loss: 0.08069365471601486

 loss: 0.08750423789024353

 loss: 0.08894310146570206

 loss: 0.08072958886623383

 loss: 0.08365337550640106

 loss: 0.07909836620092392

 loss: 0.08539329469203949

 loss: 0.08861160278320312

 loss: 0.08555947989225388

 loss: 0.08241342753171921

 loss: 0.08891912549734116

 loss: 0.08462602645158768

 loss: 0.08035276085138321

 loss: 0.07675190269947052

 loss: 0.08295919746160507

 loss: 0.07745297998189926

 loss: 0.07691579312086105

 loss: 0.08264799416065216

 loss: 0.08678663522005081

 loss: 0.07848085463047028

 loss: 0.0809047594666481

 loss: 0.0768643468618393

 loss: 0.07797042280435562

 loss: 0.08351919800043106

 loss: 0.08400747925043106

 loss: 0.07815998047590256

 loss: 0.07471112906932831

 loss: 0.08323287963867188

 loss: 0.08102264255285263

 loss: 0.08546970039606094

 loss: 0.07637768238782883

 loss: 0.07854631543159485

 loss: 0.0740988627076149

 loss: 0.07913230359554291

 loss: 0.07172954827547073

 loss: 0.08025333285331726

 loss: 0.0818207636475563

 loss: 0.07878176867961884

 loss: 0.07538071274757385

 loss: 0.07322558760643005

 loss: 0.07339533418416977

 loss: 0.07113203406333923

 loss: 0.07310741394758224

 loss: 0.07533103227615356

 loss: 0.07406268268823624

 loss: 0.07648885995149612

 loss: 0.07933466136455536

 loss: 0.07694462686777115

 loss: 0.08504390716552734

 loss: 0.07878610491752625

 loss: 0.06983290612697601

 loss: 0.07944680005311966

 loss: 0.07220056653022766

 loss: 0.07625196129083633

 loss: 0.07443486154079437

 loss: 0.07496125251054764

 loss: 0.06978798657655716

 loss: 0.07689687609672546

 loss: 0.07202467322349548

 loss: 0.07283464074134827

 loss: 0.06276088953018188

 loss: 0.0718047022819519

 loss: 0.07385361194610596

 loss: 0.0790165364742279

 loss: 0.06778371334075928

 loss: 0.07129498571157455

 loss: 0.08147267997264862

 loss: 0.07644426077604294

 loss: 0.07100757956504822

 loss: 0.07021015137434006

 loss: 0.07391046732664108

 loss: 0.06995470076799393

 loss: 0.07205382734537125

 loss: 0.06842254847288132

 loss: 0.07226824015378952

 loss: 0.07461393624544144

 loss: 0.07328970730304718

 loss: 0.06956914067268372

 loss: 0.07490701973438263

 loss: 0.07435820251703262

 loss: 0.06928599625825882

 loss: 0.06916091591119766

 loss: 0.07264986634254456

 loss: 0.07003866881132126

 loss: 0.07121280580759048

 loss: 0.07194926589727402

 loss: 0.0666952133178711

 loss: 0.06839299201965332

 loss: 0.06856664270162582

 loss: 0.06712077558040619

 loss: 0.07114923000335693

 loss: 0.05980581417679787

 loss: 0.06741649657487869

 loss: 0.06040411815047264

 loss: 0.06517062336206436

 loss: 0.07034369558095932

 loss: 0.0674525648355484

 loss: 0.06436597555875778

 loss: 0.059898920357227325

 loss: 0.06588439643383026

 loss: 0.06071002781391144

 loss: 0.061693720519542694

 loss: 0.0693620815873146

 loss: 0.06833896785974503

 loss: 0.07502161711454391

 loss: 0.07120904326438904

 loss: 0.07300917059183121

 loss: 0.06808331608772278

 loss: 0.06259842216968536

 loss: 0.07280196994543076

 loss: 0.06809616088867188

 loss: 0.05729197338223457

 loss: 0.06922339648008347

 loss: 0.06317117065191269

 loss: 0.06597794592380524

 loss: 0.062305182218551636

 loss: 0.0662204921245575

 loss: 0.06943383812904358

 loss: 0.06770984828472137

 loss: 0.06605483591556549

 loss: 0.06128304451704025

 loss: 0.06770805269479752

 loss: 0.06539943814277649

 loss: 0.07012834399938583

 loss: 0.06558937579393387

 loss: 0.05976250022649765

 loss: 0.06337271630764008

 loss: 0.05959125980734825

 loss: 0.06227667257189751

 loss: 0.06363855302333832

 loss: 0.06428077816963196

 loss: 0.06761348247528076

 loss: 0.06216253712773323

 loss: 0.06057925149798393

 loss: 0.06192028149962425

 loss: 0.06707540154457092

 loss: 0.05816738307476044

 loss: 0.06276095658540726

 loss: 0.06171576678752899

 loss: 0.06390494853258133

 loss: 0.05612177029252052

 loss: 0.06558315455913544

 loss: 0.06589340418577194

 loss: 0.06372354924678802

 loss: 0.06039774790406227

 loss: 0.059614866971969604

 loss: 0.06505792587995529

 loss: 0.05667103826999664

 loss: 0.05971263349056244

 loss: 0.05788527429103851

 loss: 0.06405948847532272

 loss: 0.0663004219532013

 loss: 0.060063838958740234

 loss: 0.05826340615749359

 loss: 0.05602401867508888

 loss: 0.0598207488656044

 loss: 0.06723081320524216

 loss: 0.06557436287403107

 loss: 0.05962677299976349

 loss: 0.053052667528390884

 loss: 0.06317850947380066

 loss: 0.055099520832300186

 loss: 0.059434451162815094

 loss: 0.058608002960681915

 loss: 0.055099233984947205

 loss: 0.05716927349567413

 loss: 0.058637261390686035

 loss: 0.06096212565898895

 loss: 0.05659512057900429

 loss: 0.05800332501530647

 loss: 0.06203359737992287

 loss: 0.0594736710190773

 loss: 0.06062357872724533

 loss: 0.06338782608509064

 loss: 0.05801096931099892

 loss: 0.060588158667087555

 loss: 0.055304840207099915

 loss: 0.062101103365421295

 loss: 0.05564381554722786

 loss: 0.056844521313905716

 loss: 0.0644184872508049

 loss: 0.05666177719831467

 loss: 0.054230909794569016

 loss: 0.06007474288344383

 loss: 0.054544467478990555

 loss: 0.06000179424881935

 loss: 0.0540538989007473

 loss: 0.05256141722202301

 loss: 0.05773431807756424

 loss: 0.054820556193590164

 loss: 0.05762140452861786

 loss: 0.0563068687915802

 loss: 0.05119326710700989

 loss: 0.04952162131667137

 loss: 0.059393033385276794

 loss: 0.06178876757621765

 loss: 0.05543988198041916

 loss: 0.057112738490104675

 loss: 0.054712727665901184

 loss: 0.05739450827240944

 loss: 0.05700397491455078

 loss: 0.05615813285112381

 loss: 0.054165199398994446

 loss: 0.05938110873103142

 loss: 0.05089784786105156

 loss: 0.052018024027347565

 loss: 0.059697527438402176

 loss: 0.05602991208434105

 loss: 0.059513941407203674

 loss: 0.05492687225341797

 loss: 0.051295794546604156

 loss: 0.0497879683971405

 loss: 0.051066890358924866

 loss: 0.052023306488990784

 loss: 0.055078521370887756

 loss: 0.05656271427869797

 loss: 0.05054865777492523

 loss: 0.04965551197528839

 loss: 0.053422268480062485

 loss: 0.05305705964565277

 loss: 0.04997424781322479

 loss: 0.04763135313987732

 loss: 0.05300629511475563

 loss: 0.05759326368570328

 loss: 0.05749168246984482

 loss: 0.05458235368132591

 loss: 0.05277002230286598

 loss: 0.05866096541285515

 loss: 0.05016167461872101

 loss: 0.05723528936505318

 loss: 0.052628230303525925

 loss: 0.054841384291648865

 loss: 0.05254369601607323

 loss: 0.053544048219919205

 loss: 0.051379166543483734

 loss: 0.04908040165901184

 loss: 0.05367768183350563

 loss: 0.05076243355870247

 loss: 0.050401248037815094

 loss: 0.05168144404888153

 loss: 0.049551382660865784

 loss: 0.05239586904644966

 loss: 0.05283820256590843

 loss: 0.05100381001830101

 loss: 0.0494096465408802

 loss: 0.05082166567444801

 loss: 0.05294680595397949

 loss: 0.05540352687239647

 loss: 0.05407343804836273

 loss: 0.048875972628593445

 loss: 0.05234060436487198

 loss: 0.048994168639183044

 loss: 0.05583077296614647

 loss: 0.05320100858807564

 loss: 0.050573159009218216

 loss: 0.05106638744473457

 loss: 0.052773937582969666

 loss: 0.047225285321474075

 loss: 0.05036529526114464

 loss: 0.05059345066547394

 loss: 0.04994787275791168

 loss: 0.04964223876595497

 loss: 0.051441460847854614

 loss: 0.05335787311196327

 loss: 0.054598771035671234

 loss: 0.05103753134608269

 loss: 0.053166743367910385

 loss: 0.04460591450333595

 loss: 0.05082365497946739

 loss: 0.057438455522060394

 loss: 0.05074937641620636

 loss: 0.051525335758924484

 loss: 0.05178746208548546

 loss: 0.05050003528594971

 loss: 0.05097351223230362

 loss: 0.04725303500890732

 loss: 0.04996450990438461

 loss: 0.05058169364929199

 loss: 0.04970547556877136

 loss: 0.04889128729701042

 loss: 0.04779359698295593

 loss: 0.048289213329553604

 loss: 0.04994586855173111

 loss: 0.05085044726729393

 loss: 0.05128184333443642

 loss: 0.04418356716632843

 loss: 0.04673001170158386

 loss: 0.04828030243515968

 loss: 0.05115976929664612

 loss: 0.05071166157722473

 loss: 0.04745477810502052

 loss: 0.047027844935655594

 loss: 0.047537632286548615

 loss: 0.04225130379199982

 loss: 0.04722656309604645

 loss: 0.04932776466012001

 loss: 0.043722547590732574

 loss: 0.0474313460290432

 loss: 0.04430463910102844

 loss: 0.04621759057044983

 loss: 0.04395577311515808

 loss: 0.04684629291296005

 loss: 0.04618838429450989

 loss: 0.04899571090936661

 loss: 0.052105262875556946

 loss: 0.04901382327079773

 loss: 0.04601285979151726

 loss: 0.049508124589920044

 loss: 0.04784749820828438

 loss: 0.04648413509130478

 loss: 0.046127110719680786

 loss: 0.045810237526893616

 loss: 0.04744913429021835

 loss: 0.050171513110399246

 loss: 0.047961506992578506

 loss: 0.042975977063179016

 loss: 0.04513045400381088

 loss: 0.044683389365673065

 loss: 0.04838861525058746

 loss: 0.041408322751522064

 loss: 0.04830552265048027

 loss: 0.0457569882273674

 loss: 0.044850483536720276

 loss: 0.04583747684955597

 loss: 0.048329953104257584

 loss: 0.04615672677755356

 loss: 0.04571937397122383

 loss: 0.0427863784134388

 loss: 0.04395752772688866

 loss: 0.04041467607021332

 loss: 0.046944759786129

 loss: 0.04848784580826759

 loss: 0.04340686649084091

 loss: 0.044855937361717224

 loss: 0.0410258024930954

 loss: 0.0440414622426033

 loss: 0.044012948870658875

 loss: 0.041499748826026917

 loss: 0.04267795383930206

 loss: 0.04360631853342056

 loss: 0.044782914221286774

 loss: 0.04608319327235222

 loss: 0.04160051420331001

 loss: 0.04954802617430687

 loss: 0.04402642324566841

 loss: 0.03993970900774002

 loss: 0.04092036187648773

 loss: 0.04163011536002159

 loss: 0.04595966264605522

 loss: 0.039302267134189606

 loss: 0.03851323202252388

 loss: 0.043216295540332794

 loss: 0.044912420213222504

 loss: 0.04307592287659645

 loss: 0.04255318269133568

 loss: 0.040803298354148865

 loss: 0.04488283023238182

 loss: 0.042517371475696564

 loss: 0.039983175694942474

 loss: 0.04621446132659912

 loss: 0.045332469046115875

 loss: 0.04263801500201225

 loss: 0.0411519892513752

 loss: 0.0436333566904068

 loss: 0.04417085275053978

 loss: 0.039830781519412994

 loss: 0.04757136479020119

 loss: 0.03920666128396988

 loss: 0.04010733217000961

 loss: 0.04066486284136772

 loss: 0.04256239905953407

 loss: 0.04185914620757103

 loss: 0.04259958118200302

 loss: 0.039285365492105484

 loss: 0.03985634446144104

 loss: 0.04018468037247658

 loss: 0.04261394590139389

 loss: 0.03905217722058296

 loss: 0.0450156070291996

 loss: 0.04169319197535515

 loss: 0.044894102960824966

 loss: 0.042586278170347214

 loss: 0.04103736951947212

 loss: 0.043944790959358215

 loss: 0.04242105409502983

 loss: 0.039297763258218765

 loss: 0.04086293652653694

 loss: 0.04021177440881729

 loss: 0.04126589745283127

 loss: 0.0405796617269516

 loss: 0.04159930348396301

 loss: 0.04547973722219467

 loss: 0.042340874671936035

 loss: 0.03892636299133301

 loss: 0.040609393268823624

 loss: 0.037912774831056595

 loss: 0.03994021564722061

 loss: 0.04205744341015816

 loss: 0.04368958622217178

 loss: 0.040995560586452484

 loss: 0.040127769112586975

 loss: 0.04186626523733139

 loss: 0.038096584379673004

 loss: 0.0419490709900856

 loss: 0.04146718978881836

 loss: 0.04253324493765831

 loss: 0.04276224225759506

 loss: 0.0373210608959198

 loss: 0.04181477054953575

 loss: 0.03952440246939659

 loss: 0.04305465146899223

 loss: 0.03683781623840332

 loss: 0.039959296584129333

 loss: 0.04097511246800423

 loss: 0.03957752138376236

 loss: 0.03933825343847275

 loss: 0.03731228783726692

 loss: 0.03971278667449951

 loss: 0.03919307887554169

 loss: 0.038296185433864594

 loss: 0.04328671470284462

 loss: 0.04160255566239357

 loss: 0.041302796453237534

 loss: 0.04105191305279732

 loss: 0.041433125734329224

 loss: 0.04139474034309387

 loss: 0.03745326027274132

 loss: 0.03761729598045349

 loss: 0.03906784579157829

 loss: 0.03615858405828476

 loss: 0.0401388518512249

 loss: 0.04201919585466385

 loss: 0.03790542483329773

 loss: 0.04085546359419823

 loss: 0.04038093611598015

 loss: 0.03458547219634056

 loss: 0.03424272686243057

 loss: 0.03529655933380127

 loss: 0.03733661025762558

 loss: 0.03361062332987785

 loss: 0.04090273752808571

 loss: 0.03754638880491257

 loss: 0.03881470859050751

 loss: 0.036016952246427536

 loss: 0.0323416106402874

Beginning logging procedure...

Collecting data for eval...
[array([[ 0.11823672,  0.15299486, -0.6225364 , -0.3858533 , -0.7331701 ,
         0.04954936,  0.69987357,  0.27701348]], dtype=float32)]
Traceback (most recent call last):
  File "cs285/scripts/run_hw1_behavior_cloning.py", line 120, in <module>
    main()
  File "cs285/scripts/run_hw1_behavior_cloning.py", line 117, in main
    trainer.run_training_loop()
  File "cs285/scripts/run_hw1_behavior_cloning.py", line 51, in run_training_loop
    expert_policy=self.loaded_expert_policy,
  File "/home/cilab/Desktop/homework_fall2019/hw1/cs285/infrastructure/rl_trainer.py", line 146, in run_training_loop
    self.perform_logging(itr, paths, eval_policy, train_video_paths)
  File "/home/cilab/Desktop/homework_fall2019/hw1/cs285/infrastructure/rl_trainer.py", line 226, in perform_logging
    eval_paths, eval_envsteps_this_batch = sample_trajectories(self.env, eval_policy, self.params['eval_batch_size'], self.params['ep_len'])
  File "/home/cilab/Desktop/homework_fall2019/hw1/cs285/infrastructure/utils.py", line 68, in sample_trajectories
    path = sample_trajectory(env, policy, max_path_length, render)
  File "/home/cilab/Desktop/homework_fall2019/hw1/cs285/infrastructure/utils.py", line 33, in sample_trajectory
    raise
RuntimeError: No active exception to reraise
